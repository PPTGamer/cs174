{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2 CS174\n",
    "---\n",
    "The following problem set will focus on the lessons on Numpy and advanced Python functions. Answer the following guide questions by writing functions or code in cells that accomplish the requirements.\n",
    "\n",
    "1. Create a function that takes in 3 parameters (integer n, integer a, integer b), and generates an nxn numpy matrix with random values from the range a to b inclusive. **2 pts.**\n",
    "\n",
    "2. Present a count plot of the top 20 occurring words in **data/networks.txt**. Use the stopword file **data/stop.txt** to filter out the stopwords **2 pts.**\n",
    "\n",
    "3. Using **data/ateneo.txt** and **data/tl-en.txt**, convert the phrases into straight english phrases. Leave words with no translation be. **6 pts.**\n",
    "\n",
    "4. Using **data/tl-en.txt**, create a one-to-one English to Filipino translation of your favorite English song. Develop functions to handle pluralities, and multiple entry results. Discuss your code and please put at least 1-liner docs for the functions. **10 pts.**\n",
    "       \n",
    "### Bonus:\n",
    "Repeat #4 but do a Filipino to English translation of your favorite Filipno song. Same specs apply. **5 pts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deadline **Feb 20 11:59PM**. Submission link to be posted in Moodle. \n",
    "Submit the .ipynb file only with name <SURNAME>_<ID NUMBER>.ipynb. \n",
    "    \n",
    "Sample: **\"BAUTISTA_110464.ipynb\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "Create a function that takes in 3 parameters (integer n, integer a, integer b), and generates an nxn numpy matrix with random values from the range a to b inclusive. **2 pts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomMatrix(n, a, b):\n",
    "    b_prime = np.nextafter(b,b+1)  # [a, b') = [a,b]\n",
    "    return np.random.rand(n,n)*(b_prime-a) + a #[0,1) * (b'-a) = [0, b-a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.22270715 2.25740677 1.71807484]\n",
      " [8.02344449 7.31448418 4.84282115]\n",
      " [5.82114077 5.30075703 8.70500362]]\n",
      "[[20. 20. 20. 20.]\n",
      " [20. 20. 20. 20.]\n",
      " [20. 20. 20. 20.]\n",
      " [20. 20. 20. 20.]]\n",
      "[[10.43401928 10.63788951 10.65909067 10.15723623 10.96072802]\n",
      " [10.46045527 10.4255212  10.09889436 10.45278497 10.87162717]\n",
      " [10.72480084 10.525563   10.73338321 10.04533044 10.82388852]\n",
      " [10.02401134 10.86339664 10.15075957 10.73923193 10.047331  ]\n",
      " [10.63472981 10.78779814 10.20641468 10.77771651 10.4926622 ]]\n"
     ]
    }
   ],
   "source": [
    "print(randomMatrix(3,1,10))\n",
    "print(randomMatrix(4,20,20))\n",
    "print(randomMatrix(5,10,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "Present a count plot of the top 20 occurring words in **data/networks.txt**. Use the stopword file **data/stop.txt** to filter out the stopwords **2 pts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function cleans uses whitepace and punctuation to convert an array of lines of text into an array of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def getWordList(lines):\n",
    "    wordlist = []\n",
    "    for line in lines:\n",
    "        # split to remove whitespace.\n",
    "        for word in line.split():\n",
    "            # transform to lowercase, remove leading/trailing punctuation.\n",
    "            word = word.lower().strip(string.punctuation)\n",
    "            wordlist.append(word)\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the wordlist of `networks.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/networks.txt', 'r')\n",
    "num2_data = f.readlines()\n",
    "f.close()\n",
    "num2_word_list = getWordList(num2_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate all the words into a single string, so we can leave the word splitting to a Python package. This cleans up _some_ of the dirty data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'disadvantagesandadvantagesofnetworkssincethefirstc'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num2_joined = ''.join(num2_word_list)\n",
    "num2_joined[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use WordNinja (install: `pip install wordninja`) to probabilistically perform word segmentation based on a built-in model of word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordninja'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-67ee123ca363>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mwordninja\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnum2_word_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordninja\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum2_joined\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnum2_word_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordninja'"
     ]
    }
   ],
   "source": [
    "import wordninja\n",
    "num2_word_list = wordninja.split(num2_joined)\n",
    "num2_word_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we filter out the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/stop.txt', 'r')\n",
    "stop_words_data = f.readlines()\n",
    "f.close()\n",
    "stop_words = getWordList(stop_words_data)\n",
    "num2_word_list = list(filter(lambda x: x not in stop_words, num2_word_list))\n",
    "num2_word_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use modified versions of the functions from PS1 to present a count plot of the top 20 occurring words in **data/networks.txt**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def getFreqDict(wordlist):\n",
    "    frequency_dict = {}\n",
    "    for word in wordlist:\n",
    "        if word in frequency_dict:\n",
    "            frequency_dict[word] += 1\n",
    "        else:\n",
    "            frequency_dict[word] = 1\n",
    "    return frequency_dict\n",
    "\n",
    "def plotFreqDict(frequency_dict, lim = None):\n",
    "    words, freqs = zip(*frequency_dict.items()) \n",
    "\n",
    "    if lim is None:\n",
    "        lim = range(len(frequency_dict))\n",
    "    indSort = np.argsort(freqs)\n",
    "    words = np.array(words)[indSort]\n",
    "    freqs = np.array(freqs)[indSort]\n",
    "\n",
    "    \n",
    "    plt.barh(range(len(frequency_dict)), freqs,zorder=3)\n",
    "    plt.grid(True,zorder=0)\n",
    "    plt.yticks(range(len(words)), words)\n",
    "    plt.ylim(len(words)-lim-0.5,len(words))\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.plot()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.title('Count plot of top 20 occuring words in networks.txt')\n",
    "plotFreqDict(getFreqDict(num2_word_list), lim=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "Using **data/ateneo.txt** and **data/tl-en.txt**, convert the phrases into straight english phrases. Leave words with no translation be. **6 pts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General plan: Use NLTK collocation features to guess the correct work if there is a one to many mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data files. For the TL->EN dictionary, to handle one-to-many mappings, we create a dictionary which maps a TL word to a list of EN candidate translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/ateneo.txt', 'r')\n",
    "num3_ateneo = f.readlines()\n",
    "f.close()\n",
    "f = open('data/tl-en.txt', 'r', errors='ignore')\n",
    "tlen_data = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlen_dictionary = {}\n",
    "for pair in tlen_data:\n",
    "    tl, en, *rest = pair.split()\n",
    "    if tl in tlen_dictionary:\n",
    "        tlen_dictionary[tl].append(en)\n",
    "    else:\n",
    "        tlen_dictionary[tl] = [en]\n",
    "tlen_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We translate the sentences using the following steps.\n",
    "1. For every sentence, we first split into words, then apply the dictionary.\n",
    " \n",
    "2. If a word in the sentence matches a TL word in the dictionary, we first replace it with the list of possible EN candidates.\n",
    "\n",
    "3. If there is only one EN candidate, we apply the dictionary directly. We do this first in order to increase the English information in the sentence, which would assist in the next step.\n",
    "\n",
    "4. To handle cases where there are multiple EN translations for a single TL word, we choose a word depending on the correlation of each EN candidate with the other EN words in the sentence. \n",
    "\n",
    "5. We restore the original punctuation and capitalization of the sentence by looking at the formatting of the corresponding word in the source sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/a/29933716"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lstripped(s):\n",
    "    s = \"-?-\" + s\n",
    "    return s.replace(s.lstrip(string.punctuation), '')\n",
    "\n",
    "def rstripped(s):\n",
    "    s = s + \"-?-\"\n",
    "    return s.replace(s.rstrip(string.punctuation), '')\n",
    "\n",
    "def stripped(s):\n",
    "    return lstripped(s), rstripped(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_format(source, pattern):\n",
    "    if pattern.strip(string.punctuation)[0].isupper():\n",
    "        return lstripped(pattern) + source[0].upper() + source[1:] + rstripped(pattern)\n",
    "    else:\n",
    "        return lstripped(pattern) + source + rstripped(pattern)\n",
    "                                                       \n",
    "def choose_translation(candidates, sentence):\n",
    "    return candidates[0]\n",
    "\n",
    "for sentence in num3_ateneo:\n",
    "    print(\"Sentence:\", sentence.strip(string.whitespace))\n",
    "    raw_word_list = sentence.strip(string.whitespace).split()\n",
    "    word_list = [x.strip(string.punctuation).lower() for x in raw_word_list]\n",
    "    print(\"Step 1:\\t\", word_list)\n",
    "    translated_sentence = [tlen_dictionary[x]  if x in tlen_dictionary else x for x in word_list]\n",
    "    print(\"Step 2:\\t\", translated_sentence)\n",
    "    \n",
    "    translated_sentence = [x[0] if isinstance(x,list) and len(x)==1 else x for x in translated_sentence]\n",
    "    print(\"Step 3:\\t\", translated_sentence)\n",
    "    \n",
    "    translated_sentence = [choose_translation(x,translated_sentence) if isinstance(x,list) else x for x in translated_sentence]\n",
    "    print(\"Step 4:\\t\", translated_sentence)\n",
    "    \n",
    "    translated_sentence = ' '.join([restore_format(translated_sentence[i], raw_word_list[i]) for i in range(len(raw_word_list))])\n",
    "    print(\"Step 5:\\t\", translated_sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "Using **data/tl-en.txt**, create a one-to-one English to Filipino translation of your favorite English song. Develop functions to handle pluralities, and multiple entry results. Discuss your code and please put at least 1-liner docs for the functions. **10 pts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geese goose True\n",
      "mice mouse True\n",
      "bars bar True\n",
      "foos foos False\n",
      "foo foo False\n",
      "families family True\n",
      "family family False\n",
      "dog dog False\n",
      "dogs dog True\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def isplural(word):\n",
    "    lemma = wnl.lemmatize(word, 'n')\n",
    "    plural = True if word is not lemma else False\n",
    "    return plural, lemma\n",
    "\n",
    "nounls = ['geese', 'mice', 'bars', 'foos', 'foo', \n",
    "                'families', 'family', 'dog', 'dogs']\n",
    "\n",
    "for nn in nounls:\n",
    "    isp, lemma = isplural(nn)\n",
    "    print(nn, lemma, isp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus\n",
    "Repeat #4 but do a Filipino to English translation of your favorite Filipno song. Same specs apply. **5 pts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
